{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef1db1-03e0-498d-839a-9716a57abb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the downloading of this file\n",
      "Sharding file number 1 of 128 called C:/Users/swtir/OneDrive/Documents/Deep_Machine_Learning_ease_of_working_locally/deep-machine-learning/Final_Project/laion1B-nolang-aesthetic/part-00000-a718cdfa-8fa6-4f99-a950-2ffa6b13c6c4-c000.snappy.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File sharded in 22 shards\n",
      "Downloading starting now, check your bandwidth speed (with bwm-ng)your cpu (with htop), and your disk usage (with iotop)!\n",
      "Sharding file number 2 of 128 called C:/Users/swtir/OneDrive/Documents/Deep_Machine_Learning_ease_of_working_locally/deep-machine-learning/Final_Project/laion1B-nolang-aesthetic/part-00001-a718cdfa-8fa6-4f99-a950-2ffa6b13c6c4-c000.snappy.parquet\n",
      "File sharded in 22 shards\n",
      "Downloading starting now, check your bandwidth speed (with bwm-ng)your cpu (with htop), and your disk usage (with iotop)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [12:01:36, 2485.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharding file number 3 of 128 called C:/Users/swtir/OneDrive/Documents/Deep_Machine_Learning_ease_of_working_locally/deep-machine-learning/Final_Project/laion1B-nolang-aesthetic/part-00002-a718cdfa-8fa6-4f99-a950-2ffa6b13c6c4-c000.snappy.parquet\n",
      "File sharded in 22 shards\n",
      "Downloading starting now, check your bandwidth speed (with bwm-ng)your cpu (with htop), and your disk usage (with iotop)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [18:18:39, 499.07s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharding file number 4 of 128 called C:/Users/swtir/OneDrive/Documents/Deep_Machine_Learning_ease_of_working_locally/deep-machine-learning/Final_Project/laion1B-nolang-aesthetic/part-00003-a718cdfa-8fa6-4f99-a950-2ffa6b13c6c4-c000.snappy.parquet\n",
      "File sharded in 22 shards\n",
      "Downloading starting now, check your bandwidth speed (with bwm-ng)your cpu (with htop), and your disk usage (with iotop)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [18:28:34, 153.51s/it]"
     ]
    }
   ],
   "source": [
    "import img2dataset\n",
    "\n",
    "url_list = r\"C:\\Users\\swtir\\OneDrive\\Documents\\Deep_Machine_Learning_ease_of_working_locally\\deep-machine-learning\\Final_Project\\laion1B-nolang-aesthetic\"\n",
    "# Download and process images\n",
    "img2dataset.download(\n",
    "    url_list=url_list,\n",
    "    input_format=\"parquet\",\n",
    "    url_col=\"URL\",\n",
    "    caption_col=\"TEXT\",\n",
    "    output_format=\"files\",\n",
    "    output_folder=\"./images\",\n",
    "    processes_count=16,\n",
    "    thread_count=64,\n",
    "    image_size=512,\n",
    "    resize_only_if_bigger=True,\n",
    "    save_additional_columns=[\"aesthetic\", \"pwatermark\", \"punsafe\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ed2d33-c6ee-4380-8c74-91c4959dffcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been moved to C:\\Users\\swtir\\OneDrive\\Documents\\Deep_Machine_Learning_ease_of_working_locally\\deep-machine-learning\\Final_Project\\images2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the root directory where the subfolders are located\n",
    "root_dir = r\"C:\\Users\\swtir\\OneDrive\\Documents\\Deep_Machine_Learning_ease_of_working_locally\\deep-machine-learning\\Final_Project\\images\"\n",
    "\n",
    "# Define the destination directory where all files will be merged\n",
    "destination_dir = r\"C:\\Users\\swtir\\OneDrive\\Documents\\Deep_Machine_Learning_ease_of_working_locally\\deep-machine-learning\\Final_Project\\images2\"\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# Walk through all subfolders and move files to the destination directory\n",
    "for subdir, dirs, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(subdir, file)\n",
    "        \n",
    "        # Move each file to the destination folder\n",
    "        shutil.move(file_path, os.path.join(destination_dir, file))\n",
    "\n",
    "print(f\"All files have been moved to {destination_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee250098-37e0-45b6-987b-2a19192ae27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': 'C:\\\\Users\\\\swtir\\\\OneDrive\\\\Documents\\\\Deep_Machine_Learning_ease_of_working_locally\\\\deep-machine-learning\\\\Final_Project\\\\images2\\\\000050205.jpg', 'caption': 'Best Ever No-Bake Cheesecake'}\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory where the images and captions are stored\n",
    "data_dir = r\"C:\\Users\\swtir\\OneDrive\\Documents\\Deep_Machine_Learning_ease_of_working_locally\\deep-machine-learning\\Final_Project\\images2\"\n",
    "\n",
    "# Step 1: Collect data from the directory\n",
    "def collect_data(data_dir):\n",
    "    data_entries = []\n",
    "\n",
    "    # Iterate over the files in the directory\n",
    "    for filename in os.listdir(data_dir):\n",
    "        # We only care about the base filename (without extension) to match image and txt\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            base_filename = os.path.splitext(filename)[0]\n",
    "            \n",
    "            # Construct the paths for the image and caption\n",
    "            image_path = os.path.join(data_dir, f\"{base_filename}.jpg\")\n",
    "            caption_path = os.path.join(data_dir, f\"{base_filename}.txt\")\n",
    "            \n",
    "            # Read the image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Read the caption with encoding handling\n",
    "            caption = \"\"\n",
    "            try:\n",
    "                # Try reading as utf-8 first\n",
    "                with open(caption_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    caption = f.read().strip()\n",
    "            except UnicodeDecodeError:\n",
    "                # Fallback to 'latin1' or 'ISO-8859-1' encoding\n",
    "                with open(caption_path, \"r\", encoding=\"latin1\") as f:\n",
    "                    caption = f.read().strip()\n",
    "            \n",
    "            # Create an entry combining the image path and the caption\n",
    "            data_entries.append({\n",
    "                \"image_path\": image_path,\n",
    "                \"caption\": caption\n",
    "            })\n",
    "    \n",
    "    return data_entries\n",
    "\n",
    "# Step 2: Collect the data\n",
    "data = collect_data(data_dir)\n",
    "\n",
    "# Step 3: Convert the data to a pandas DataFrame (or use directly with Hugging Face Dataset)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 4: Convert the DataFrame to Hugging Face Dataset (if needed)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Step 5: Verify the dataset structure by printing a sample\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4299b09-f94e-4459-9b93-e86826cf3085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3488eb3278844deca5fa4de28e3621d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/346339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Save the DataFrame (df)\n",
    "df.to_pickle(\"collected_data.pkl\")  # Save DataFrame as a pickle file\n",
    "df.to_csv(\"collected_data.csv\", index=False)  # Alternatively, save as a CSV file\n",
    "\n",
    "# Step 2: Save the Hugging Face Dataset\n",
    "dataset.save_to_disk(\"huggingface_dataset\")  # Save the dataset to disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
