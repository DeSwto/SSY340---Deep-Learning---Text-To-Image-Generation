{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b962aace-68fd-4269-95e3-fb121bbd0384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid image-caption pairs: 5653\n",
      "Total number of batches: 354\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "import numpy\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.utils as nn_utils  # For gradient norm computation\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import UNet2DConditionModel, StableDiffusionPipeline\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get all .jpg files and check for corresponding non-empty .txt files\n",
    "        self.image_filenames = sorted([f for f in os.listdir(data_dir) if f.endswith(\".jpg\")])\n",
    "        \n",
    "        # Filter out pairs with empty or missing captions\n",
    "        self.image_filenames = [\n",
    "            f for f in self.image_filenames \n",
    "            if os.path.exists(os.path.join(data_dir, f.replace(\".jpg\", \".txt\"))) \n",
    "            and os.path.getsize(os.path.join(data_dir, f.replace(\".jpg\", \".txt\"))) > 0\n",
    "        ]\n",
    "        \n",
    "        print(f\"Total valid image-caption pairs: {len(self.image_filenames)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_filenames[idx]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "    \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading image {img_name}: {e}\")\n",
    "    \n",
    "        # Load corresponding caption\n",
    "        caption_file = img_name.replace(\".jpg\", \".txt\")\n",
    "        caption_path = os.path.join(self.data_dir, caption_file)\n",
    "    \n",
    "        try:\n",
    "            with open(caption_path, \"r\") as file:\n",
    "                caption = file.read().strip()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading caption {caption_file}: {e}\")\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "    \n",
    "        return image, caption\n",
    "\n",
    "\n",
    "# Load the pre-trained CLIP model\n",
    "device = \"cuda\" \n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "# Paths to your image and caption directories\n",
    "data_dir = r\"C:\\Users\\swtir\\OneDrive\\Documents\\Deep_Machine_Learning_ease_of_working_locally\\deep-machine-learning\\Final_Project\\images_small\"\n",
    "\n",
    "# Define image transformations (resizing, normalizing as per CLIP's expected input)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Ensure the right size for the model\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader with optimized loading\n",
    "small_dataset = ImageTextDataset(data_dir=data_dir, transform=transform)\n",
    "dataloader = DataLoader(small_dataset, batch_size=16, shuffle=True, pin_memory=True)\n",
    "\n",
    "total_batches = len(dataloader)\n",
    "print(f\"Total number of batches: {total_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d0860e-d920-4d5a-9f80-4f1d1a3b498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_clip(model, processor, dataloader, epochs, learning_rate=1e-5, accumulation_steps=15, print_every=1, lr_warmup=1000):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Scheduler to gradually warm up learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(step / lr_warmup, 1.0))\n",
    "    \n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()  # Mixed-precision training\n",
    "\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        for i, (images, captions) in enumerate(dataloader):\n",
    "            if images is None or captions is None:\n",
    "                print(f\"Warning: No data received at batch {i+1}\")\n",
    "                continue\n",
    "\n",
    "            # Move data to device and truncate text input to max_length=77 tokens\n",
    "            inputs = processor(text=captions, images=images, return_tensors=\"pt\", padding=True, truncation=True, max_length=77).to(device)\n",
    "            pixel_values = inputs['pixel_values']\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.cuda.amp.autocast():  # Automatic mixed-precision\n",
    "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, attention_mask=attention_mask, return_loss=True)\n",
    "                loss = outputs.loss / accumulation_steps\n",
    "\n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Gradient accumulation and update\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)  # Gradient scaling to avoid NaNs\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            if print_every and (i + 1) % print_every == 0:\n",
    "                print(f\"Batch {i+1}/{len(dataloader)}, Loss: {loss.item() * accumulation_steps:.4f}\")\n",
    "        \n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed. Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"Fine-tuning completed.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Set up the model, processor, optimizer, and fine-tuning process\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Assuming your ImageTextDataset is correctly defined and data loaded\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(small_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Fine-tune the CLIP model\n",
    "fine_tuned_model = fine_tune_clip(model, processor, dataloader, epochs=1, learning_rate=1e-5, accumulation_steps=8, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef90244d-39a3-4c60-a23f-4668d921875b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a999e50bd8ee4955b7717a58f052a3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swtir\\anaconda3\\envs\\dml\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\swtir\\anaconda3\\envs\\dml\\Lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d5ad160d814a6d93bd3c4b5c807db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to get text embeddings from the CLIP model\n",
    "def get_text_embedding(text, clip_model, device):\n",
    "    # Tokenize and encode the text with the CLIP model\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "    text_embedding = clip_model.encode_text(text_tokens)\n",
    "    \n",
    "    # Normalize the embedding\n",
    "    text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return text_embedding\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Load CLIP model\n",
    "clip_model, _ = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "# Load pre-trained Stable Diffusion pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(device)\n",
    "\n",
    "# Disable safety checker\n",
    "pipe.safety_checker = None\n",
    "\n",
    "# Example text prompt\n",
    "text_prompt = \"A monkey on a tree smoking\"\n",
    "\n",
    "# Step 1: Get the text embedding from the CLIP model\n",
    "text_embedding = get_text_embedding(text_prompt, clip_model, device)\n",
    "text_embedding = text_embedding.to(torch.float32)  # Ensure float32 dtype\n",
    "\n",
    "# Step 2: Adjust embedding shape for cross-attention in Stable Diffusion\n",
    "batch_size = 1\n",
    "sequence_length = pipe.tokenizer.model_max_length  # Sequence length expected by Stable Diffusion (usually 77)\n",
    "embedding_dim = text_embedding.shape[-1]\n",
    "\n",
    "# Expand the dimensions to match [batch_size, sequence_length, embedding_dim]\n",
    "text_embedding = text_embedding.unsqueeze(0).expand(batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "# Create a dummy negative prompt embedding for classifier-free guidance\n",
    "negative_prompt = \"\"\n",
    "negative_embedding = get_text_embedding(negative_prompt, clip_model, device)\n",
    "negative_embedding = negative_embedding.to(torch.float32).unsqueeze(0).expand(batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "# Step 3: Use the pipeline for generating the image with prompt embeddings\n",
    "with torch.no_grad():\n",
    "    generated_images = pipe(\n",
    "        prompt_embeds=text_embedding,\n",
    "        negative_prompt_embeds=negative_embedding,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.5,  # Adjust guidance scale as needed\n",
    "    ).images\n",
    "\n",
    "# Step 4: Display the generated image\n",
    "generated_image = generated_images[0]\n",
    "generated_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0666a86-21ec-42be-bc7b-ea3a4e9fd2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Score: 0.1252\n",
      "CLIP Score: 0.1044\n",
      "CLIP Score: 0.1229\n",
      "CLIP Score: 0.1398\n",
      "CLIP Score: 0.0885\n",
      "CLIP Score: 0.1332\n",
      "CLIP Score: 0.1327\n",
      "CLIP Score: 0.1402\n",
      "CLIP Score: 0.1023\n",
      "CLIP Score: 0.1168\n",
      "CLIP Score: 0.1251\n",
      "CLIP Score: 0.1132\n",
      "CLIP Score: 0.1195\n",
      "CLIP Score: 0.1107\n",
      "CLIP Score: 0.1277\n",
      "CLIP Score: 0.1505\n",
      "CLIP Score: 0.1088\n",
      "CLIP Score: 0.1397\n",
      "CLIP Score: 0.1199\n",
      "CLIP Score: 0.1401\n",
      "CLIP Score: 0.1209\n",
      "CLIP Score: 0.1158\n",
      "CLIP Score: 0.1217\n",
      "CLIP Score: 0.1243\n",
      "CLIP Score: 0.1217\n",
      "CLIP Score: 0.1273\n",
      "CLIP Score: 0.1434\n",
      "CLIP Score: 0.1034\n",
      "CLIP Score: 0.1426\n",
      "CLIP Score: 0.1335\n",
      "CLIP Score: 0.1100\n",
      "CLIP Score: 0.1272\n",
      "CLIP Score: 0.1347\n",
      "CLIP Score: 0.1311\n",
      "CLIP Score: 0.1136\n",
      "CLIP Score: 0.0857\n",
      "CLIP Score: 0.1270\n",
      "CLIP Score: 0.1305\n",
      "CLIP Score: 0.1397\n",
      "CLIP Score: 0.1193\n",
      "CLIP Score: 0.1290\n",
      "CLIP Score: 0.1303\n",
      "CLIP Score: 0.1345\n",
      "CLIP Score: 0.1412\n",
      "CLIP Score: 0.1215\n",
      "CLIP Score: 0.1583\n",
      "CLIP Score: 0.1507\n",
      "CLIP Score: 0.1301\n",
      "CLIP Score: 0.1265\n",
      "CLIP Score: 0.1517\n",
      "CLIP Score: 0.1402\n",
      "CLIP Score: 0.1427\n",
      "CLIP Score: 0.1435\n",
      "CLIP Score: 0.1140\n",
      "CLIP Score: 0.1388\n",
      "CLIP Score: 0.1429\n",
      "CLIP Score: 0.1178\n",
      "CLIP Score: 0.1461\n",
      "CLIP Score: 0.1223\n",
      "CLIP Score: 0.1301\n",
      "CLIP Score: 0.1292\n",
      "CLIP Score: 0.1355\n",
      "CLIP Score: 0.1390\n",
      "CLIP Score: 0.1313\n",
      "CLIP Score: 0.1661\n",
      "CLIP Score: 0.1292\n",
      "CLIP Score: 0.0938\n",
      "CLIP Score: 0.1380\n",
      "CLIP Score: 0.1349\n",
      "CLIP Score: 0.1154\n",
      "CLIP Score: 0.1350\n",
      "CLIP Score: 0.1320\n",
      "CLIP Score: 0.1605\n",
      "CLIP Score: 0.1544\n",
      "CLIP Score: 0.1570\n",
      "CLIP Score: 0.1179\n",
      "CLIP Score: 0.1364\n",
      "CLIP Score: 0.1480\n",
      "CLIP Score: 0.1543\n",
      "CLIP Score: 0.1213\n",
      "CLIP Score: 0.1282\n",
      "CLIP Score: 0.1349\n",
      "CLIP Score: 0.1414\n",
      "CLIP Score: 0.1502\n",
      "CLIP Score: 0.1194\n",
      "CLIP Score: 0.1338\n",
      "CLIP Score: 0.1419\n",
      "CLIP Score: 0.1262\n",
      "CLIP Score: 0.1465\n",
      "CLIP Score: 0.1206\n",
      "CLIP Score: 0.1387\n",
      "CLIP Score: 0.1251\n",
      "CLIP Score: 0.1687\n",
      "CLIP Score: 0.1352\n",
      "CLIP Score: 0.1495\n",
      "CLIP Score: 0.1570\n",
      "CLIP Score: 0.1357\n",
      "CLIP Score: 0.1388\n",
      "CLIP Score: 0.1483\n",
      "CLIP Score: 0.1326\n",
      "CLIP Score: 0.1337\n",
      "CLIP Score: 0.1343\n",
      "CLIP Score: 0.1652\n",
      "CLIP Score: 0.1500\n",
      "CLIP Score: 0.1551\n",
      "CLIP Score: 0.1395\n",
      "CLIP Score: 0.1479\n",
      "CLIP Score: 0.1365\n",
      "CLIP Score: 0.1466\n",
      "CLIP Score: 0.1521\n",
      "CLIP Score: 0.1429\n",
      "CLIP Score: 0.1371\n",
      "CLIP Score: 0.1427\n",
      "CLIP Score: 0.1448\n",
      "CLIP Score: 0.1445\n",
      "CLIP Score: 0.1362\n",
      "CLIP Score: 0.1582\n",
      "CLIP Score: 0.1586\n",
      "CLIP Score: 0.1373\n",
      "CLIP Score: 0.1504\n",
      "CLIP Score: 0.1548\n",
      "CLIP Score: 0.1247\n",
      "CLIP Score: 0.1383\n",
      "CLIP Score: 0.1355\n",
      "CLIP Score: 0.1263\n",
      "CLIP Score: 0.1270\n",
      "CLIP Score: 0.1270\n",
      "CLIP Score: 0.1435\n",
      "CLIP Score: 0.1342\n",
      "CLIP Score: 0.1457\n",
      "CLIP Score: 0.1707\n",
      "CLIP Score: 0.1400\n",
      "CLIP Score: 0.1266\n",
      "CLIP Score: 0.1289\n",
      "CLIP Score: 0.1404\n",
      "CLIP Score: 0.1390\n",
      "CLIP Score: 0.1262\n",
      "CLIP Score: 0.1475\n",
      "CLIP Score: 0.1432\n",
      "CLIP Score: 0.1365\n",
      "CLIP Score: 0.1402\n",
      "CLIP Score: 0.1207\n",
      "CLIP Score: 0.1142\n",
      "CLIP Score: 0.1619\n",
      "CLIP Score: 0.1530\n",
      "CLIP Score: 0.1485\n",
      "CLIP Score: 0.1249\n",
      "CLIP Score: 0.1406\n",
      "CLIP Score: 0.1440\n",
      "CLIP Score: 0.1575\n",
      "CLIP Score: 0.1424\n",
      "CLIP Score: 0.1285\n",
      "CLIP Score: 0.1283\n",
      "CLIP Score: 0.1494\n",
      "CLIP Score: 0.1494\n",
      "CLIP Score: 0.1811\n",
      "CLIP Score: 0.1272\n",
      "CLIP Score: 0.1680\n",
      "CLIP Score: 0.1398\n",
      "CLIP Score: 0.1329\n",
      "CLIP Score: 0.1274\n",
      "CLIP Score: 0.1483\n",
      "CLIP Score: 0.1701\n",
      "CLIP Score: 0.1394\n",
      "CLIP Score: 0.1186\n",
      "CLIP Score: 0.1134\n",
      "CLIP Score: 0.1277\n",
      "CLIP Score: 0.1321\n",
      "CLIP Score: 0.1400\n",
      "CLIP Score: 0.1146\n",
      "CLIP Score: 0.1547\n",
      "CLIP Score: 0.1282\n",
      "CLIP Score: 0.1438\n",
      "CLIP Score: 0.1379\n",
      "CLIP Score: 0.1156\n",
      "CLIP Score: 0.1304\n",
      "CLIP Score: 0.1580\n",
      "CLIP Score: 0.1333\n",
      "CLIP Score: 0.1334\n",
      "CLIP Score: 0.1222\n",
      "CLIP Score: 0.1252\n",
      "CLIP Score: 0.1666\n",
      "CLIP Score: 0.1634\n",
      "CLIP Score: 0.1449\n",
      "CLIP Score: 0.1487\n",
      "CLIP Score: 0.1380\n",
      "CLIP Score: 0.1307\n",
      "CLIP Score: 0.1292\n",
      "CLIP Score: 0.1612\n",
      "CLIP Score: 0.1397\n",
      "CLIP Score: 0.1490\n",
      "CLIP Score: 0.1296\n",
      "CLIP Score: 0.1581\n",
      "CLIP Score: 0.1137\n",
      "CLIP Score: 0.1411\n",
      "CLIP Score: 0.1321\n",
      "CLIP Score: 0.1712\n",
      "CLIP Score: 0.1421\n",
      "CLIP Score: 0.1340\n",
      "CLIP Score: 0.1340\n",
      "CLIP Score: 0.1383\n",
      "CLIP Score: 0.1578\n",
      "CLIP Score: 0.1713\n",
      "CLIP Score: 0.1287\n",
      "CLIP Score: 0.1258\n",
      "CLIP Score: 0.1119\n",
      "CLIP Score: 0.1233\n",
      "CLIP Score: 0.1563\n",
      "CLIP Score: 0.1062\n",
      "CLIP Score: 0.1684\n",
      "CLIP Score: 0.1208\n",
      "CLIP Score: 0.1402\n",
      "CLIP Score: 0.1227\n",
      "CLIP Score: 0.1364\n",
      "CLIP Score: 0.1101\n",
      "CLIP Score: 0.1388\n",
      "CLIP Score: 0.1315\n",
      "CLIP Score: 0.1200\n",
      "CLIP Score: 0.1221\n",
      "CLIP Score: 0.1523\n",
      "CLIP Score: 0.1475\n",
      "CLIP Score: 0.1407\n",
      "CLIP Score: 0.1296\n",
      "CLIP Score: 0.1788\n",
      "CLIP Score: 0.1214\n",
      "CLIP Score: 0.1532\n",
      "CLIP Score: 0.1357\n",
      "CLIP Score: 0.1122\n",
      "CLIP Score: 0.1670\n",
      "CLIP Score: 0.1343\n",
      "CLIP Score: 0.1231\n",
      "CLIP Score: 0.1263\n",
      "CLIP Score: 0.1305\n",
      "CLIP Score: 0.1378\n",
      "CLIP Score: 0.1268\n",
      "CLIP Score: 0.1103\n",
      "CLIP Score: 0.1452\n",
      "CLIP Score: 0.1541\n",
      "CLIP Score: 0.1373\n",
      "CLIP Score: 0.1394\n",
      "CLIP Score: 0.1123\n",
      "CLIP Score: 0.1586\n",
      "CLIP Score: 0.1484\n",
      "CLIP Score: 0.1350\n",
      "CLIP Score: 0.1529\n",
      "CLIP Score: 0.1318\n",
      "CLIP Score: 0.1363\n",
      "CLIP Score: 0.1531\n",
      "CLIP Score: 0.1467\n",
      "CLIP Score: 0.1609\n",
      "CLIP Score: 0.1298\n",
      "CLIP Score: 0.1219\n",
      "CLIP Score: 0.1082\n",
      "CLIP Score: 0.1502\n",
      "CLIP Score: 0.1310\n",
      "CLIP Score: 0.1098\n",
      "CLIP Score: 0.1270\n",
      "CLIP Score: 0.1356\n",
      "CLIP Score: 0.1710\n",
      "CLIP Score: 0.1719\n",
      "CLIP Score: 0.1325\n",
      "CLIP Score: 0.1211\n",
      "CLIP Score: 0.1385\n",
      "CLIP Score: 0.1139\n",
      "CLIP Score: 0.1292\n",
      "CLIP Score: 0.1244\n",
      "CLIP Score: 0.1325\n",
      "CLIP Score: 0.1401\n",
      "CLIP Score: 0.1405\n",
      "CLIP Score: 0.1317\n",
      "CLIP Score: 0.0839\n",
      "CLIP Score: 0.1411\n",
      "CLIP Score: 0.1101\n",
      "CLIP Score: 0.1510\n",
      "CLIP Score: 0.1337\n",
      "CLIP Score: 0.1322\n",
      "CLIP Score: 0.1445\n",
      "CLIP Score: 0.1243\n",
      "CLIP Score: 0.1435\n",
      "CLIP Score: 0.1380\n",
      "CLIP Score: 0.1419\n",
      "CLIP Score: 0.1499\n",
      "CLIP Score: 0.1404\n",
      "CLIP Score: 0.1008\n",
      "CLIP Score: 0.0994\n",
      "CLIP Score: 0.1065\n",
      "CLIP Score: 0.1389\n",
      "CLIP Score: 0.1414\n",
      "CLIP Score: 0.1118\n",
      "CLIP Score: 0.1354\n",
      "CLIP Score: 0.1205\n",
      "CLIP Score: 0.1352\n",
      "CLIP Score: 0.1258\n",
      "CLIP Score: 0.1319\n",
      "CLIP Score: 0.1082\n",
      "CLIP Score: 0.1523\n",
      "CLIP Score: 0.1354\n",
      "CLIP Score: 0.1038\n",
      "CLIP Score: 0.1281\n",
      "CLIP Score: 0.1315\n",
      "CLIP Score: 0.1324\n",
      "CLIP Score: 0.1357\n",
      "CLIP Score: 0.1571\n",
      "CLIP Score: 0.1393\n",
      "CLIP Score: 0.1118\n",
      "CLIP Score: 0.1486\n",
      "CLIP Score: 0.1445\n",
      "CLIP Score: 0.1455\n",
      "CLIP Score: 0.1158\n",
      "CLIP Score: 0.1176\n",
      "CLIP Score: 0.1332\n",
      "CLIP Score: 0.1578\n",
      "CLIP Score: 0.1517\n",
      "CLIP Score: 0.1431\n",
      "CLIP Score: 0.1350\n",
      "CLIP Score: 0.1057\n",
      "CLIP Score: 0.1435\n",
      "CLIP Score: 0.1408\n",
      "CLIP Score: 0.1237\n",
      "CLIP Score: 0.1175\n",
      "CLIP Score: 0.1420\n",
      "CLIP Score: 0.1317\n",
      "CLIP Score: 0.1479\n",
      "CLIP Score: 0.1329\n",
      "CLIP Score: 0.1533\n",
      "CLIP Score: 0.1320\n",
      "CLIP Score: 0.1565\n",
      "CLIP Score: 0.1465\n",
      "CLIP Score: 0.1406\n",
      "CLIP Score: 0.1311\n",
      "CLIP Score: 0.1412\n",
      "CLIP Score: 0.1270\n",
      "CLIP Score: 0.1462\n",
      "CLIP Score: 0.1462\n",
      "CLIP Score: 0.1254\n",
      "CLIP Score: 0.1554\n",
      "CLIP Score: 0.1302\n",
      "CLIP Score: 0.1254\n",
      "CLIP Score: 0.1409\n",
      "CLIP Score: 0.1466\n",
      "CLIP Score: 0.1353\n",
      "CLIP Score: 0.1391\n",
      "CLIP Score: 0.1393\n",
      "CLIP Score: 0.1125\n",
      "CLIP Score: 0.1110\n",
      "CLIP Score: 0.1129\n",
      "CLIP Score: 0.1380\n",
      "CLIP Score: 0.1325\n",
      "CLIP Score: 0.1318\n",
      "CLIP Score: 0.1464\n",
      "CLIP Score: 0.1261\n",
      "CLIP Score: 0.1289\n",
      "CLIP Score: 0.1396\n",
      "CLIP Score: 0.1286\n",
      "CLIP Score: 0.1280\n",
      "CLIP Score: 0.1070\n",
      "CLIP Score: 0.1293\n",
      "CLIP Score: 0.1410\n",
      "CLIP Score: 0.1536\n",
      "CLIP Score: 0.1352\n",
      "CLIP Score: 0.1337\n",
      "CLIP Score: 0.1419\n",
      "CLIP Score: 0.1328\n",
      "CLIP Score: 0.1365\n",
      "CLIP Score: 0.1526\n",
      "CLIP Score: 0.1535\n",
      "CLIP Score: 0.1241\n",
      "CLIP Score: 0.1260\n",
      "CLIP Score: 0.1236\n",
      "CLIP Score: 0.1308\n",
      "CLIP Score: 0.1510\n",
      "CLIP Score: 0.1462\n",
      "CLIP Score: 0.1330\n",
      "CLIP Score: 0.1299\n",
      "CLIP Score: 0.1536\n",
      "CLIP Score: 0.1315\n",
      "CLIP Score: 0.1718\n",
      "CLIP Score: 0.1314\n",
      "CLIP Score: 0.1358\n",
      "CLIP Score: 0.1209\n",
      "CLIP Score: 0.1491\n",
      "CLIP Score: 0.1227\n",
      "CLIP Score: 0.1441\n",
      "CLIP Score: 0.1277\n",
      "CLIP Score: 0.1189\n",
      "CLIP Score: 0.1189\n",
      "CLIP Score: 0.1397\n",
      "CLIP Score: 0.1413\n",
      "CLIP Score: 0.1187\n",
      "CLIP Score: 0.1595\n",
      "CLIP Score: 0.1647\n",
      "CLIP Score: 0.1303\n",
      "CLIP Score: 0.1273\n",
      "CLIP Score: 0.1427\n",
      "CLIP Score: 0.1466\n",
      "CLIP Score: 0.1564\n",
      "CLIP Score: 0.1503\n",
      "CLIP Score: 0.1339\n",
      "CLIP Score: 0.1610\n",
      "CLIP Score: 0.1509\n",
      "CLIP Score: 0.1485\n",
      "CLIP Score: 0.1187\n",
      "CLIP Score: 0.1724\n",
      "CLIP Score: 0.1351\n",
      "CLIP Score: 0.1354\n",
      "CLIP Score: 0.1344\n",
      "CLIP Score: 0.1281\n",
      "CLIP Score: 0.1372\n",
      "CLIP Score: 0.1640\n",
      "CLIP Score: 0.1224\n",
      "CLIP Score: 0.1201\n",
      "CLIP Score: 0.1378\n",
      "CLIP Score: 0.1391\n",
      "CLIP Score: 0.1085\n",
      "CLIP Score: 0.1203\n",
      "CLIP Score: 0.1437\n",
      "CLIP Score: 0.1613\n",
      "CLIP Score: 0.1319\n",
      "CLIP Score: 0.1153\n",
      "CLIP Score: 0.1399\n",
      "CLIP Score: 0.1541\n",
      "CLIP Score: 0.1399\n",
      "CLIP Score: 0.1205\n",
      "CLIP Score: 0.1261\n",
      "CLIP Score: 0.1277\n",
      "CLIP Score: 0.1472\n",
      "CLIP Score: 0.1688\n",
      "CLIP Score: 0.1272\n",
      "CLIP Score: 0.1463\n",
      "CLIP Score: 0.1304\n",
      "CLIP Score: 0.1460\n",
      "CLIP Score: 0.1378\n",
      "CLIP Score: 0.1399\n",
      "CLIP Score: 0.1397\n",
      "CLIP Score: 0.1380\n",
      "CLIP Score: 0.1142\n",
      "CLIP Score: 0.1446\n",
      "CLIP Score: 0.1782\n",
      "CLIP Score: 0.1460\n",
      "CLIP Score: 0.1382\n",
      "CLIP Score: 0.1566\n",
      "CLIP Score: 0.1457\n",
      "CLIP Score: 0.1659\n",
      "CLIP Score: 0.1453\n",
      "CLIP Score: 0.1667\n",
      "CLIP Score: 0.1485\n",
      "CLIP Score: 0.1571\n",
      "CLIP Score: 0.1339\n",
      "CLIP Score: 0.1337\n",
      "CLIP Score: 0.1511\n",
      "CLIP Score: 0.1404\n",
      "CLIP Score: 0.1658\n",
      "CLIP Score: 0.1354\n",
      "CLIP Score: 0.1633\n",
      "CLIP Score: 0.1233\n",
      "CLIP Score: 0.1505\n",
      "CLIP Score: 0.1393\n",
      "CLIP Score: 0.1254\n",
      "CLIP Score: 0.1217\n",
      "CLIP Score: 0.1316\n",
      "CLIP Score: 0.1324\n",
      "CLIP Score: 0.1250\n",
      "CLIP Score: 0.1510\n",
      "CLIP Score: 0.1396\n",
      "CLIP Score: 0.1481\n",
      "CLIP Score: 0.1738\n",
      "CLIP Score: 0.1384\n",
      "CLIP Score: 0.1399\n",
      "CLIP Score: 0.1253\n",
      "CLIP Score: 0.1673\n",
      "CLIP Score: 0.1257\n",
      "CLIP Score: 0.1268\n",
      "CLIP Score: 0.1269\n",
      "CLIP Score: 0.1300\n",
      "CLIP Score: 0.1367\n",
      "CLIP Score: 0.1671\n",
      "CLIP Score: 0.1505\n",
      "CLIP Score: 0.1501\n",
      "CLIP Score: 0.1413\n",
      "CLIP Score: 0.1298\n",
      "CLIP Score: 0.1547\n",
      "CLIP Score: 0.1507\n",
      "CLIP Score: 0.1329\n",
      "CLIP Score: 0.1497\n",
      "CLIP Score: 0.1424\n",
      "CLIP Score: 0.1290\n",
      "CLIP Score: 0.1486\n",
      "CLIP Score: 0.1371\n",
      "CLIP Score: 0.1316\n",
      "CLIP Score: 0.1388\n",
      "CLIP Score: 0.1506\n",
      "CLIP Score: 0.1640\n",
      "CLIP Score: 0.1327\n",
      "CLIP Score: 0.1133\n",
      "CLIP Score: 0.1244\n",
      "CLIP Score: 0.1248\n",
      "CLIP Score: 0.1274\n",
      "CLIP Score: 0.1248\n",
      "CLIP Score: 0.1973\n",
      "CLIP Score: 0.1749\n",
      "CLIP Score: 0.1435\n",
      "CLIP Score: 0.1368\n",
      "CLIP Score: 0.1321\n",
      "CLIP Score: 0.1240\n",
      "CLIP Score: 0.1702\n",
      "CLIP Score: 0.1467\n",
      "CLIP Score: 0.1173\n",
      "CLIP Score: 0.1300\n",
      "CLIP Score: 0.1293\n",
      "CLIP Score: 0.1447\n",
      "CLIP Score: 0.1422\n",
      "CLIP Score: 0.1413\n",
      "CLIP Score: 0.1396\n",
      "CLIP Score: 0.1204\n",
      "CLIP Score: 0.1320\n",
      "CLIP Score: 0.1600\n",
      "CLIP Score: 0.1413\n",
      "CLIP Score: 0.1392\n",
      "CLIP Score: 0.1412\n",
      "CLIP Score: 0.1737\n",
      "CLIP Score: 0.1177\n",
      "CLIP Score: 0.1330\n",
      "CLIP Score: 0.1472\n",
      "CLIP Score: 0.1473\n",
      "CLIP Score: 0.1067\n",
      "CLIP Score: 0.1663\n",
      "CLIP Score: 0.1736\n",
      "CLIP Score: 0.1479\n",
      "CLIP Score: 0.1241\n",
      "CLIP Score: 0.1301\n",
      "CLIP Score: 0.1498\n",
      "CLIP Score: 0.1378\n",
      "CLIP Score: 0.1646\n",
      "CLIP Score: 0.1364\n",
      "CLIP Score: 0.1333\n",
      "CLIP Score: 0.1425\n",
      "CLIP Score: 0.1490\n",
      "CLIP Score: 0.1507\n",
      "CLIP Score: 0.1324\n",
      "CLIP Score: 0.1471\n",
      "CLIP Score: 0.1446\n",
      "CLIP Score: 0.1503\n",
      "CLIP Score: 0.1526\n",
      "CLIP Score: 0.1444\n",
      "CLIP Score: 0.1427\n",
      "CLIP Score: 0.1686\n",
      "CLIP Score: 0.1379\n",
      "CLIP Score: 0.1304\n",
      "CLIP Score: 0.1585\n",
      "CLIP Score: 0.1455\n",
      "CLIP Score: 0.1418\n",
      "CLIP Score: 0.1765\n",
      "CLIP Score: 0.1349\n",
      "CLIP Score: 0.1602\n",
      "CLIP Score: 0.1416\n",
      "CLIP Score: 0.1354\n",
      "CLIP Score: 0.1521\n",
      "CLIP Score: 0.1479\n",
      "CLIP Score: 0.1303\n",
      "CLIP Score: 0.1591\n",
      "CLIP Score: 0.1702\n",
      "CLIP Score: 0.1574\n",
      "CLIP Score: 0.1483\n",
      "CLIP Score: 0.1433\n",
      "CLIP Score: 0.1322\n",
      "CLIP Score: 0.1342\n",
      "CLIP Score: 0.1290\n",
      "CLIP Score: 0.1236\n",
      "CLIP Score: 0.1285\n",
      "CLIP Score: 0.1409\n",
      "CLIP Score: 0.1384\n",
      "CLIP Score: 0.1384\n",
      "CLIP Score: 0.1701\n",
      "CLIP Score: 0.1742\n",
      "CLIP Score: 0.1616\n",
      "CLIP Score: 0.1422\n",
      "CLIP Score: 0.1525\n",
      "CLIP Score: 0.1473\n",
      "CLIP Score: 0.1438\n",
      "CLIP Score: 0.1307\n",
      "CLIP Score: 0.1543\n",
      "CLIP Score: 0.1322\n",
      "CLIP Score: 0.1459\n",
      "CLIP Score: 0.1201\n",
      "CLIP Score: 0.1513\n",
      "CLIP Score: 0.1244\n",
      "CLIP Score: 0.1316\n",
      "CLIP Score: 0.1541\n",
      "CLIP Score: 0.1287\n",
      "CLIP Score: 0.1395\n",
      "CLIP Score: 0.1254\n",
      "CLIP Score: 0.1417\n",
      "CLIP Score: 0.1294\n",
      "CLIP Score: 0.1502\n",
      "CLIP Score: 0.1495\n",
      "CLIP Score: 0.1357\n",
      "CLIP Score: 0.1282\n",
      "CLIP Score: 0.1660\n",
      "CLIP Score: 0.1175\n",
      "CLIP Score: 0.1419\n",
      "CLIP Score: 0.1472\n",
      "CLIP Score: 0.1296\n",
      "CLIP Score: 0.1446\n",
      "CLIP Score: 0.1368\n",
      "CLIP Score: 0.1400\n",
      "CLIP Score: 0.1731\n",
      "CLIP Score: 0.1419\n",
      "CLIP Score: 0.1286\n",
      "CLIP Score: 0.1386\n",
      "CLIP Score: 0.1118\n",
      "CLIP Score: 0.1563\n",
      "CLIP Score: 0.1344\n",
      "CLIP Score: 0.1341\n",
      "CLIP Score: 0.1270\n",
      "CLIP Score: 0.1535\n",
      "CLIP Score: 0.1044\n",
      "CLIP Score: 0.1257\n",
      "CLIP Score: 0.1680\n",
      "CLIP Score: 0.1516\n",
      "CLIP Score: 0.1386\n",
      "CLIP Score: 0.1471\n",
      "CLIP Score: 0.1559\n",
      "CLIP Score: 0.1309\n",
      "CLIP Score: 0.1487\n",
      "CLIP Score: 0.1615\n",
      "CLIP Score: 0.1535\n",
      "CLIP Score: 0.1320\n",
      "CLIP Score: 0.1291\n",
      "CLIP Score: 0.1481\n",
      "CLIP Score: 0.1486\n",
      "CLIP Score: 0.1596\n",
      "CLIP Score: 0.1489\n",
      "CLIP Score: 0.1354\n",
      "CLIP Score: 0.1615\n",
      "CLIP Score: 0.1359\n",
      "CLIP Score: 0.1461\n",
      "CLIP Score: 0.1361\n",
      "CLIP Score: 0.1578\n",
      "CLIP Score: 0.1347\n",
      "CLIP Score: 0.1567\n",
      "CLIP Score: 0.1504\n",
      "CLIP Score: 0.1059\n",
      "CLIP Score: 0.1237\n",
      "CLIP Score: 0.1620\n",
      "CLIP Score: 0.1679\n",
      "CLIP Score: 0.1381\n",
      "CLIP Score: 0.1477\n",
      "CLIP Score: 0.1425\n",
      "CLIP Score: 0.1531\n",
      "CLIP Score: 0.1524\n",
      "CLIP Score: 0.1745\n",
      "CLIP Score: 0.1359\n",
      "CLIP Score: 0.1364\n",
      "CLIP Score: 0.1106\n",
      "CLIP Score: 0.1304\n",
      "CLIP Score: 0.1606\n",
      "CLIP Score: 0.1774\n",
      "CLIP Score: 0.1701\n",
      "CLIP Score: 0.1352\n",
      "CLIP Score: 0.1269\n",
      "CLIP Score: 0.1470\n",
      "CLIP Score: 0.1543\n",
      "CLIP Score: 0.1389\n",
      "CLIP Score: 0.1410\n",
      "CLIP Score: 0.1448\n",
      "CLIP Score: 0.1522\n",
      "CLIP Score: 0.1614\n",
      "CLIP Score: 0.1448\n",
      "CLIP Score: 0.1443\n",
      "CLIP Score: 0.1386\n",
      "CLIP Score: 0.1245\n",
      "CLIP Score: 0.1565\n",
      "CLIP Score: 0.1381\n",
      "CLIP Score: 0.1478\n",
      "CLIP Score: 0.1493\n",
      "CLIP Score: 0.1654\n",
      "CLIP Score: 0.1348\n",
      "CLIP Score: 0.1443\n",
      "CLIP Score: 0.1489\n",
      "CLIP Score: 0.1438\n",
      "CLIP Score: 0.1329\n",
      "CLIP Score: 0.1594\n",
      "CLIP Score: 0.1659\n",
      "CLIP Score: 0.1549\n",
      "CLIP Score: 0.1217\n",
      "CLIP Score: 0.1443\n",
      "CLIP Score: 0.1592\n",
      "CLIP Score: 0.1454\n",
      "CLIP Score: 0.1603\n",
      "CLIP Score: 0.1536\n",
      "CLIP Score: 0.1337\n",
      "CLIP Score: 0.1538\n",
      "CLIP Score: 0.1372\n",
      "CLIP Score: 0.1596\n",
      "CLIP Score: 0.1529\n",
      "CLIP Score: 0.1374\n",
      "CLIP Score: 0.1468\n",
      "CLIP Score: 0.1409\n",
      "CLIP Score: 0.1532\n",
      "CLIP Score: 0.1559\n",
      "CLIP Score: 0.1684\n",
      "CLIP Score: 0.1463\n",
      "CLIP Score: 0.1688\n",
      "CLIP Score: 0.1710\n",
      "CLIP Score: 0.1092\n",
      "CLIP Score: 0.1249\n",
      "CLIP Score: 0.1497\n",
      "CLIP Score: 0.1298\n",
      "CLIP Score: 0.1721\n",
      "CLIP Score: 0.1452\n",
      "CLIP Score: 0.1577\n",
      "CLIP Score: 0.1546\n",
      "CLIP Score: 0.1467\n",
      "CLIP Score: 0.1495\n",
      "CLIP Score: 0.1618\n",
      "CLIP Score: 0.1432\n",
      "CLIP Score: 0.1461\n",
      "CLIP Score: 0.1615\n",
      "CLIP Score: 0.1633\n",
      "CLIP Score: 0.1365\n",
      "CLIP Score: 0.1384\n",
      "CLIP Score: 0.1476\n",
      "CLIP Score: 0.1516\n",
      "CLIP Score: 0.1551\n",
      "CLIP Score: 0.1537\n",
      "CLIP Score: 0.1566\n",
      "CLIP Score: 0.1429\n",
      "CLIP Score: 0.1388\n",
      "CLIP Score: 0.1558\n",
      "CLIP Score: 0.1561\n",
      "CLIP Score: 0.1359\n",
      "CLIP Score: 0.1374\n",
      "CLIP Score: 0.1610\n",
      "CLIP Score: 0.1686\n",
      "CLIP Score: 0.1390\n",
      "CLIP Score: 0.1390\n",
      "CLIP Score: 0.1608\n",
      "CLIP Score: 0.1306\n",
      "CLIP Score: 0.1312\n",
      "CLIP Score: 0.1333\n",
      "CLIP Score: 0.1321\n",
      "CLIP Score: 0.1381\n",
      "CLIP Score: 0.1414\n",
      "CLIP Score: 0.1424\n",
      "CLIP Score: 0.1511\n",
      "CLIP Score: 0.1416\n",
      "CLIP Score: 0.1413\n",
      "CLIP Score: 0.1930\n",
      "CLIP Score: 0.1510\n",
      "CLIP Score: 0.1050\n",
      "CLIP Score: 0.1290\n",
      "CLIP Score: 0.1473\n",
      "CLIP Score: 0.1505\n",
      "CLIP Score: 0.1414\n",
      "CLIP Score: 0.1662\n",
      "CLIP Score: 0.1274\n",
      "CLIP Score: 0.1336\n",
      "CLIP Score: 0.1633\n",
      "CLIP Score: 0.1283\n",
      "CLIP Score: 0.1463\n",
      "CLIP Score: 0.1171\n",
      "CLIP Score: 0.1492\n",
      "CLIP Score: 0.1294\n",
      "CLIP Score: 0.1516\n",
      "CLIP Score: 0.1654\n",
      "CLIP Score: 0.1688\n",
      "CLIP Score: 0.1487\n",
      "CLIP Score: 0.1409\n",
      "CLIP Score: 0.1195\n",
      "CLIP Score: 0.1205\n",
      "CLIP Score: 0.1380\n",
      "CLIP Score: 0.1432\n",
      "CLIP Score: 0.1466\n",
      "CLIP Score: 0.1484\n",
      "CLIP Score: 0.1509\n",
      "CLIP Score: 0.1422\n",
      "CLIP Score: 0.1518\n",
      "CLIP Score: 0.1488\n",
      "CLIP Score: 0.1447\n",
      "CLIP Score: 0.1501\n",
      "CLIP Score: 0.1518\n",
      "CLIP Score: 0.1577\n",
      "CLIP Score: 0.1468\n",
      "CLIP Score: 0.1488\n",
      "CLIP Score: 0.1494\n",
      "CLIP Score: 0.1433\n",
      "CLIP Score: 0.1471\n",
      "CLIP Score: 0.1360\n",
      "CLIP Score: 0.1704\n",
      "CLIP Score: 0.1529\n",
      "CLIP Score: 0.1539\n",
      "CLIP Score: 0.1500\n",
      "CLIP Score: 0.1517\n",
      "CLIP Score: 0.1540\n",
      "CLIP Score: 0.1511\n",
      "CLIP Score: 0.1595\n",
      "CLIP Score: 0.1625\n",
      "CLIP Score: 0.1464\n",
      "CLIP Score: 0.1791\n",
      "CLIP Score: 0.1466\n",
      "CLIP Score: 0.1488\n",
      "CLIP Score: 0.1261\n",
      "CLIP Score: 0.1617\n",
      "CLIP Score: 0.1618\n",
      "CLIP Score: 0.1673\n",
      "CLIP Score: 0.1429\n",
      "CLIP Score: 0.1560\n",
      "CLIP Score: 0.1313\n",
      "CLIP Score: 0.1356\n",
      "CLIP Score: 0.1379\n",
      "CLIP Score: 0.1386\n",
      "CLIP Score: 0.1323\n",
      "CLIP Score: 0.1246\n",
      "CLIP Score: 0.1461\n",
      "CLIP Score: 0.1314\n",
      "CLIP Score: 0.1725\n",
      "CLIP Score: 0.1556\n",
      "CLIP Score: 0.1551\n",
      "CLIP Score: 0.1275\n",
      "CLIP Score: 0.1504\n",
      "CLIP Score: 0.1275\n",
      "CLIP Score: 0.1564\n",
      "CLIP Score: 0.1200\n",
      "CLIP Score: 0.1326\n",
      "CLIP Score: 0.1603\n",
      "CLIP Score: 0.1368\n",
      "CLIP Score: 0.1461\n",
      "CLIP Score: 0.1483\n",
      "CLIP Score: 0.1448\n",
      "CLIP Score: 0.1611\n",
      "CLIP Score: 0.1188\n",
      "CLIP Score: 0.1566\n",
      "CLIP Score: 0.1399\n",
      "CLIP Score: 0.1345\n",
      "CLIP Score: 0.1649\n",
      "CLIP Score: 0.1518\n",
      "CLIP Score: 0.1363\n",
      "CLIP Score: 0.1268\n",
      "CLIP Score: 0.1534\n",
      "CLIP Score: 0.1401\n",
      "CLIP Score: 0.1435\n",
      "CLIP Score: 0.1692\n",
      "CLIP Score: 0.1762\n",
      "CLIP Score: 0.1330\n",
      "CLIP Score: 0.1487\n",
      "CLIP Score: 0.1400\n",
      "CLIP Score: 0.1219\n",
      "CLIP Score: 0.1662\n",
      "CLIP Score: 0.1405\n",
      "CLIP Score: 0.1394\n",
      "CLIP Score: 0.1584\n",
      "CLIP Score: 0.1440\n",
      "CLIP Score: 0.1156\n",
      "CLIP Score: 0.1181\n",
      "CLIP Score: 0.1458\n",
      "CLIP Score: 0.1533\n",
      "CLIP Score: 0.1639\n",
      "CLIP Score: 0.1508\n",
      "CLIP Score: 0.1556\n",
      "CLIP Score: 0.1533\n",
      "CLIP Score: 0.1586\n",
      "CLIP Score: 0.1510\n",
      "CLIP Score: 0.1697\n",
      "CLIP Score: 0.1872\n",
      "CLIP Score: 0.1410\n",
      "CLIP Score: 0.1685\n",
      "CLIP Score: 0.1510\n",
      "CLIP Score: 0.1514\n",
      "CLIP Score: 0.1642\n",
      "CLIP Score: 0.1464\n",
      "CLIP Score: 0.1620\n",
      "CLIP Score: 0.1551\n",
      "CLIP Score: 0.1332\n",
      "CLIP Score: 0.1466\n",
      "CLIP Score: 0.1588\n",
      "CLIP Score: 0.1359\n",
      "CLIP Score: 0.1425\n",
      "CLIP Score: 0.1415\n",
      "CLIP Score: 0.1456\n",
      "CLIP Score: 0.1749\n",
      "CLIP Score: 0.1437\n",
      "CLIP Score: 0.1496\n",
      "CLIP Score: 0.1369\n",
      "CLIP Score: 0.1461\n",
      "CLIP Score: 0.1540\n",
      "CLIP Score: 0.1331\n",
      "CLIP Score: 0.1755\n",
      "CLIP Score: 0.1431\n",
      "CLIP Score: 0.1598\n",
      "CLIP Score: 0.1414\n",
      "CLIP Score: 0.1522\n",
      "CLIP Score: 0.1609\n",
      "CLIP Score: 0.1530\n",
      "CLIP Score: 0.1634\n",
      "CLIP Score: 0.1499\n",
      "CLIP Score: 0.1542\n",
      "CLIP Score: 0.1451\n",
      "CLIP Score: 0.1602\n",
      "CLIP Score: 0.1551\n",
      "CLIP Score: 0.1497\n",
      "CLIP Score: 0.1505\n",
      "CLIP Score: 0.1610\n",
      "CLIP Score: 0.1713\n",
      "CLIP Score: 0.1566\n",
      "CLIP Score: 0.1505\n",
      "CLIP Score: 0.1445\n",
      "CLIP Score: 0.1619\n",
      "CLIP Score: 0.1501\n",
      "CLIP Score: 0.1171\n",
      "CLIP Score: 0.1612\n",
      "CLIP Score: 0.1309\n",
      "CLIP Score: 0.1452\n",
      "CLIP Score: 0.1509\n",
      "CLIP Score: 0.1365\n",
      "CLIP Score: 0.1527\n",
      "CLIP Score: 0.1527\n",
      "CLIP Score: 0.1349\n",
      "CLIP Score: 0.1513\n",
      "CLIP Score: 0.1366\n",
      "CLIP Score: 0.1250\n",
      "CLIP Score: 0.1292\n",
      "CLIP Score: 0.1536\n",
      "CLIP Score: 0.1486\n",
      "CLIP Score: 0.1306\n",
      "CLIP Score: 0.1359\n",
      "CLIP Score: 0.1425\n",
      "CLIP Score: 0.1707\n",
      "CLIP Score: 0.1618\n",
      "CLIP Score: 0.1444\n",
      "CLIP Score: 0.1609\n",
      "CLIP Score: 0.1606\n",
      "CLIP Score: 0.1617\n",
      "CLIP Score: 0.1604\n",
      "CLIP Score: 0.1343\n",
      "CLIP Score: 0.1614\n",
      "CLIP Score: 0.1559\n",
      "CLIP Score: 0.1583\n",
      "CLIP Score: 0.1659\n",
      "CLIP Score: 0.1484\n",
      "CLIP Score: 0.1365\n",
      "CLIP Score: 0.1310\n",
      "CLIP Score: 0.1568\n",
      "CLIP Score: 0.1622\n",
      "CLIP Score: 0.1468\n",
      "CLIP Score: 0.1411\n",
      "CLIP Score: 0.1409\n",
      "CLIP Score: 0.1473\n",
      "CLIP Score: 0.1747\n",
      "CLIP Score: 0.1507\n",
      "CLIP Score: 0.1881\n",
      "CLIP Score: 0.1303\n",
      "CLIP Score: 0.1630\n",
      "CLIP Score: 0.1466\n",
      "CLIP Score: 0.1551\n",
      "CLIP Score: 0.1328\n",
      "CLIP Score: 0.1388\n",
      "CLIP Score: 0.1415\n",
      "CLIP Score: 0.1357\n",
      "CLIP Score: 0.1511\n",
      "CLIP Score: 0.1817\n",
      "CLIP Score: 0.1412\n",
      "CLIP Score: 0.1420\n",
      "CLIP Score: 0.1322\n",
      "CLIP Score: 0.1388\n",
      "CLIP Score: 0.1648\n",
      "CLIP Score: 0.1574\n",
      "CLIP Score: 0.1201\n",
      "CLIP Score: 0.1573\n",
      "CLIP Score: 0.1539\n",
      "CLIP Score: 0.1534\n",
      "CLIP Score: 0.1567\n",
      "CLIP Score: 0.1525\n",
      "CLIP Score: 0.1571\n",
      "CLIP Score: 0.1402\n",
      "CLIP Score: 0.1572\n",
      "CLIP Score: 0.1417\n",
      "CLIP Score: 0.1637\n",
      "CLIP Score: 0.1645\n",
      "CLIP Score: 0.1476\n",
      "CLIP Score: 0.1651\n",
      "CLIP Score: 0.1339\n",
      "CLIP Score: 0.1552\n",
      "CLIP Score: 0.1631\n",
      "CLIP Score: 0.1322\n",
      "CLIP Score: 0.1469\n",
      "CLIP Score: 0.1524\n",
      "CLIP Score: 0.1594\n",
      "CLIP Score: 0.1651\n",
      "CLIP Score: 0.1192\n",
      "CLIP Score: 0.1289\n",
      "CLIP Score: 0.1296\n",
      "CLIP Score: 0.1745\n",
      "CLIP Score: 0.1268\n",
      "CLIP Score: 0.1387\n",
      "CLIP Score: 0.1557\n",
      "CLIP Score: 0.1188\n",
      "CLIP Score: 0.1461\n",
      "CLIP Score: 0.1443\n",
      "CLIP Score: 0.1533\n",
      "CLIP Score: 0.1541\n",
      "CLIP Score: 0.1294\n",
      "CLIP Score: 0.1711\n",
      "CLIP Score: 0.1404\n",
      "CLIP Score: 0.1336\n",
      "CLIP Score: 0.1397\n",
      "CLIP Score: 0.1427\n",
      "CLIP Score: 0.1337\n",
      "CLIP Score: 0.1446\n",
      "CLIP Score: 0.1636\n",
      "CLIP Score: 0.1466\n",
      "CLIP Score: 0.1547\n",
      "CLIP Score: 0.1615\n",
      "CLIP Score: 0.1740\n",
      "CLIP Score: 0.1658\n",
      "CLIP Score: 0.1383\n",
      "CLIP Score: 0.1409\n",
      "CLIP Score: 0.1603\n",
      "CLIP Score: 0.1370\n",
      "CLIP Score: 0.1408\n",
      "CLIP Score: 0.1687\n",
      "CLIP Score: 0.1640\n",
      "CLIP Score: 0.1639\n",
      "CLIP Score: 0.1720\n",
      "CLIP Score: 0.1523\n",
      "CLIP Score: 0.1171\n",
      "CLIP Score: 0.1453\n",
      "CLIP Score: 0.1352\n",
      "CLIP Score: 0.1491\n",
      "CLIP Score: 0.1560\n",
      "CLIP Score: 0.1308\n",
      "CLIP Score: 0.1497\n",
      "CLIP Score: 0.1480\n",
      "CLIP Score: 0.1472\n",
      "CLIP Score: 0.1617\n",
      "CLIP Score: 0.1500\n",
      "CLIP Score: 0.1534\n",
      "CLIP Score: 0.1399\n",
      "CLIP Score: 0.1475\n",
      "CLIP Score: 0.1518\n",
      "CLIP Score: 0.1602\n",
      "CLIP Score: 0.1443\n",
      "CLIP Score: 0.1578\n",
      "CLIP Score: 0.1461\n",
      "CLIP Score: 0.1487\n",
      "CLIP Score: 0.1563\n",
      "CLIP Score: 0.1344\n",
      "CLIP Score: 0.1609\n",
      "CLIP Score: 0.1313\n",
      "CLIP Score: 0.1667\n",
      "CLIP Score: 0.1549\n",
      "CLIP Score: 0.1325\n",
      "CLIP Score: 0.1195\n",
      "CLIP Score: 0.1473\n",
      "CLIP Score: 0.1393\n",
      "CLIP Score: 0.1405\n",
      "CLIP Score: 0.1468\n",
      "CLIP Score: 0.1464\n",
      "CLIP Score: 0.1604\n",
      "CLIP Score: 0.1268\n",
      "CLIP Score: 0.1430\n",
      "CLIP Score: 0.1443\n",
      "CLIP Score: 0.1392\n",
      "CLIP Score: 0.1535\n",
      "CLIP Score: 0.1459\n",
      "CLIP Score: 0.1604\n",
      "CLIP Score: 0.1608\n",
      "CLIP Score: 0.1255\n",
      "CLIP Score: 0.1487\n",
      "CLIP Score: 0.1638\n",
      "CLIP Score: 0.1459\n",
      "CLIP Score: 0.1408\n",
      "CLIP Score: 0.1367\n",
      "CLIP Score: 0.1372\n",
      "CLIP Score: 0.1288\n",
      "CLIP Score: 0.1658\n",
      "CLIP Score: 0.1539\n",
      "CLIP Score: 0.1578\n",
      "CLIP Score: 0.1611\n",
      "CLIP Score: 0.1705\n",
      "CLIP Score: 0.1409\n",
      "CLIP Score: 0.1491\n",
      "CLIP Score: 0.1547\n",
      "CLIP Score: 0.1612\n",
      "CLIP Score: 0.1272\n",
      "CLIP Score: 0.1573\n",
      "CLIP Score: 0.1338\n",
      "CLIP Score: 0.1419\n",
      "CLIP Score: 0.1538\n",
      "CLIP Score: 0.1559\n",
      "CLIP Score: 0.1609\n",
      "CLIP Score: 0.1478\n",
      "CLIP Score: 0.1411\n",
      "CLIP Score: 0.1480\n",
      "CLIP Score: 0.1324\n",
      "CLIP Score: 0.1560\n",
      "CLIP Score: 0.1428\n",
      "CLIP Score: 0.1182\n",
      "CLIP Score: 0.1584\n",
      "CLIP Score: 0.1552\n",
      "CLIP Score: 0.1381\n",
      "CLIP Score: 0.1423\n",
      "CLIP Score: 0.1470\n",
      "CLIP Score: 0.1590\n",
      "CLIP Score: 0.1766\n",
      "CLIP Score: 0.1348\n",
      "CLIP Score: 0.1539\n",
      "CLIP Score: 0.1718\n",
      "CLIP Score: 0.1385\n",
      "CLIP Score: 0.1609\n",
      "CLIP Score: 0.1526\n",
      "CLIP Score: 0.1583\n",
      "CLIP Score: 0.1512\n",
      "CLIP Score: 0.1362\n",
      "CLIP Score: 0.1530\n",
      "CLIP Score: 0.1365\n",
      "CLIP Score: 0.1414\n",
      "CLIP Score: 0.1523\n",
      "CLIP Score: 0.1625\n",
      "CLIP Score: 0.1679\n",
      "CLIP Score: 0.1460\n",
      "CLIP Score: 0.1388\n",
      "CLIP Score: 0.1663\n",
      "CLIP Score: 0.1294\n",
      "CLIP Score: 0.1502\n",
      "CLIP Score: 0.1388\n",
      "CLIP Score: 0.1401\n",
      "CLIP Score: 0.1405\n",
      "CLIP Score: 0.1456\n",
      "CLIP Score: 0.1308\n",
      "CLIP Score: 0.1498\n",
      "CLIP Score: 0.1374\n",
      "CLIP Score: 0.1489\n",
      "CLIP Score: 0.1628\n",
      "CLIP Score: 0.1590\n",
      "CLIP Score: 0.1376\n",
      "CLIP Score: 0.1509\n",
      "CLIP Score: 0.1565\n",
      "CLIP Score: 0.1421\n",
      "CLIP Score: 0.1560\n",
      "CLIP Score: 0.1505\n",
      "CLIP Score: 0.1481\n",
      "CLIP Score: 0.1564\n",
      "CLIP Score: 0.1389\n",
      "CLIP Score: 0.1747\n",
      "CLIP Score: 0.1494\n",
      "CLIP Score: 0.1495\n",
      "CLIP Score: 0.1454\n",
      "CLIP Score: 0.1545\n",
      "CLIP Score: 0.1753\n",
      "CLIP Score: 0.1553\n",
      "CLIP Score: 0.1469\n",
      "CLIP Score: 0.1414\n",
      "CLIP Score: 0.1456\n",
      "CLIP Score: 0.1497\n",
      "CLIP Score: 0.1544\n",
      "CLIP Score: 0.1574\n",
      "CLIP Score: 0.1499\n",
      "CLIP Score: 0.1532\n",
      "CLIP Score: 0.1296\n",
      "CLIP Score: 0.1483\n",
      "CLIP Score: 0.1317\n",
      "CLIP Score: 0.1401\n",
      "CLIP Score: 0.1420\n",
      "CLIP Score: 0.1515\n",
      "CLIP Score: 0.1351\n",
      "CLIP Score: 0.1379\n",
      "CLIP Score: 0.1232\n",
      "CLIP Score: 0.1449\n",
      "CLIP Score: 0.1391\n",
      "CLIP Score: 0.1603\n",
      "CLIP Score: 0.1452\n",
      "CLIP Score: 0.1090\n",
      "CLIP Score: 0.1436\n",
      "CLIP Score: 0.1552\n",
      "CLIP Score: 0.1469\n",
      "CLIP Score: 0.1631\n",
      "CLIP Score: 0.1245\n",
      "CLIP Score: 0.1589\n",
      "CLIP Score: 0.1371\n",
      "CLIP Score: 0.1717\n",
      "CLIP Score: 0.1305\n",
      "CLIP Score: 0.1523\n",
      "CLIP Score: 0.1549\n",
      "CLIP Score: 0.1253\n",
      "CLIP Score: 0.1396\n",
      "CLIP Score: 0.1346\n",
      "CLIP Score: 0.1495\n",
      "CLIP Score: 0.1578\n",
      "CLIP Score: 0.1214\n",
      "CLIP Score: 0.1623\n",
      "CLIP Score: 0.1515\n",
      "CLIP Score: 0.1601\n",
      "CLIP Score: 0.1461\n",
      "CLIP Score: 0.1474\n",
      "CLIP Score: 0.1535\n",
      "CLIP Score: 0.1731\n",
      "CLIP Score: 0.1500\n",
      "CLIP Score: 0.1688\n",
      "CLIP Score: 0.1274\n",
      "CLIP Score: 0.1203\n",
      "CLIP Score: 0.1534\n",
      "CLIP Score: 0.1643\n",
      "CLIP Score: 0.1330\n",
      "CLIP Score: 0.1453\n",
      "CLIP Score: 0.1394\n",
      "CLIP Score: 0.1561\n",
      "CLIP Score: 0.1472\n",
      "CLIP Score: 0.1506\n",
      "CLIP Score: 0.1755\n",
      "CLIP Score: 0.1417\n",
      "CLIP Score: 0.1468\n",
      "CLIP Score: 0.1493\n",
      "CLIP Score: 0.1402\n",
      "CLIP Score: 0.1392\n",
      "CLIP Score: 0.1635\n",
      "CLIP Score: 0.1495\n",
      "CLIP Score: 0.1509\n",
      "CLIP Score: 0.1447\n",
      "CLIP Score: 0.1378\n",
      "CLIP Score: 0.1438\n",
      "CLIP Score: 0.1566\n",
      "CLIP Score: 0.1412\n",
      "CLIP Score: 0.1459\n",
      "CLIP Score: 0.1364\n",
      "CLIP Score: 0.1657\n",
      "CLIP Score: 0.1584\n",
      "CLIP Score: 0.1507\n",
      "CLIP Score: 0.1608\n",
      "CLIP Score: 0.1309\n",
      "CLIP Score: 0.1221\n",
      "CLIP Score: 0.1465\n",
      "CLIP Score: 0.1470\n",
      "CLIP Score: 0.1444\n",
      "CLIP Score: 0.1419\n",
      "CLIP Score: 0.1610\n",
      "CLIP Score: 0.1522\n",
      "CLIP Score: 0.1440\n",
      "CLIP Score: 0.1353\n",
      "CLIP Score: 0.1513\n",
      "CLIP Score: 0.1483\n",
      "CLIP Score: 0.1367\n",
      "CLIP Score: 0.1310\n",
      "CLIP Score: 0.1727\n",
      "CLIP Score: 0.1555\n",
      "CLIP Score: 0.1432\n",
      "CLIP Score: 0.1313\n",
      "CLIP Score: 0.1681\n",
      "CLIP Score: 0.1544\n",
      "CLIP Score: 0.1309\n",
      "CLIP Score: 0.1593\n",
      "CLIP Score: 0.1538\n",
      "CLIP Score: 0.1610\n",
      "CLIP Score: 0.1444\n",
      "CLIP Score: 0.1371\n",
      "CLIP Score: 0.1625\n",
      "CLIP Score: 0.1463\n",
      "CLIP Score: 0.1321\n",
      "CLIP Score: 0.1619\n",
      "CLIP Score: 0.1230\n",
      "CLIP Score: 0.1525\n",
      "CLIP Score: 0.1401\n",
      "CLIP Score: 0.1728\n",
      "CLIP Score: 0.1584\n",
      "CLIP Score: 0.1220\n",
      "CLIP Score: 0.1579\n",
      "CLIP Score: 0.1664\n",
      "CLIP Score: 0.1435\n",
      "CLIP Score: 0.1479\n",
      "CLIP Score: 0.1494\n",
      "CLIP Score: 0.1543\n",
      "CLIP Score: 0.1398\n",
      "CLIP Score: 0.1713\n",
      "CLIP Score: 0.1406\n",
      "CLIP Score: 0.1461\n",
      "CLIP Score: 0.1484\n",
      "CLIP Score: 0.1632\n",
      "CLIP Score: 0.1376\n",
      "CLIP Score: 0.1476\n",
      "CLIP Score: 0.1513\n",
      "CLIP Score: 0.1503\n",
      "CLIP Score: 0.1581\n",
      "CLIP Score: 0.1497\n",
      "CLIP Score: 0.1380\n",
      "CLIP Score: 0.1861\n",
      "CLIP Score: 0.1556\n",
      "CLIP Score: 0.1613\n",
      "CLIP Score: 0.1666\n",
      "CLIP Score: 0.1432\n",
      "CLIP Score: 0.1928\n",
      "CLIP Score: 0.1662\n",
      "CLIP Score: 0.1327\n",
      "CLIP Score: 0.1476\n",
      "CLIP Score: 0.1475\n",
      "CLIP Score: 0.1439\n",
      "CLIP Score: 0.1384\n",
      "CLIP Score: 0.1493\n",
      "CLIP Score: 0.1561\n",
      "CLIP Score: 0.1544\n",
      "CLIP Score: 0.1458\n",
      "CLIP Score: 0.1474\n",
      "CLIP Score: 0.1468\n",
      "CLIP Score: 0.1528\n",
      "CLIP Score: 0.1535\n",
      "CLIP Score: 0.1722\n",
      "CLIP Score: 0.1413\n",
      "CLIP Score: 0.1295\n",
      "CLIP Score: 0.1224\n",
      "CLIP Score: 0.1408\n",
      "CLIP Score: 0.1535\n",
      "CLIP Score: 0.1473\n",
      "CLIP Score: 0.1403\n",
      "CLIP Score: 0.1633\n",
      "CLIP Score: 0.1465\n",
      "CLIP Score: 0.1334\n",
      "CLIP Score: 0.1496\n",
      "CLIP Score: 0.1754\n",
      "CLIP Score: 0.1447\n",
      "CLIP Score: 0.1472\n",
      "CLIP Score: 0.1529\n",
      "CLIP Score: 0.1498\n",
      "CLIP Score: 0.1544\n",
      "CLIP Score: 0.1639\n",
      "CLIP Score: 0.1432\n",
      "CLIP Score: 0.1643\n",
      "CLIP Score: 0.1600\n",
      "CLIP Score: 0.1558\n",
      "CLIP Score: 0.1527\n",
      "CLIP Score: 0.1668\n",
      "CLIP Score: 0.1440\n",
      "CLIP Score: 0.1635\n",
      "CLIP Score: 0.1686\n",
      "CLIP Score: 0.1491\n",
      "CLIP Score: 0.1503\n",
      "CLIP Score: 0.1571\n",
      "CLIP Score: 0.1412\n",
      "CLIP Score: 0.1397\n",
      "CLIP Score: 0.1699\n",
      "CLIP Score: 0.1534\n",
      "CLIP Score: 0.1620\n",
      "CLIP Score: 0.1557\n",
      "CLIP Score: 0.1657\n",
      "CLIP Score: 0.1455\n",
      "CLIP Score: 0.1521\n",
      "CLIP Score: 0.1411\n",
      "CLIP Score: 0.1329\n",
      "CLIP Score: 0.1633\n",
      "CLIP Score: 0.1524\n",
      "CLIP Score: 0.1513\n",
      "CLIP Score: 0.1790\n",
      "CLIP Score: 0.1466\n",
      "CLIP Score: 0.1599\n",
      "CLIP Score: 0.1462\n",
      "CLIP Score: 0.1538\n",
      "CLIP Score: 0.1429\n",
      "CLIP Score: 0.1651\n",
      "CLIP Score: 0.1716\n",
      "CLIP Score: 0.1504\n",
      "CLIP Score: 0.1615\n",
      "CLIP Score: 0.1593\n",
      "CLIP Score: 0.1373\n",
      "CLIP Score: 0.1726\n",
      "CLIP Score: 0.1462\n",
      "CLIP Score: 0.1330\n",
      "CLIP Score: 0.1482\n",
      "CLIP Score: 0.1640\n",
      "CLIP Score: 0.1416\n",
      "CLIP Score: 0.1711\n",
      "CLIP Score: 0.1531\n",
      "CLIP Score: 0.1569\n",
      "CLIP Score: 0.1570\n",
      "CLIP Score: 0.1441\n",
      "CLIP Score: 0.1353\n",
      "CLIP Score: 0.1497\n",
      "CLIP Score: 0.1365\n",
      "CLIP Score: 0.1734\n",
      "CLIP Score: 0.1581\n",
      "CLIP Score: 0.1627\n",
      "CLIP Score: 0.1585\n",
      "CLIP Score: 0.1401\n",
      "CLIP Score: 0.1542\n",
      "CLIP Score: 0.1545\n",
      "CLIP Score: 0.1298\n",
      "CLIP Score: 0.1488\n",
      "CLIP Score: 0.1683\n",
      "CLIP Score: 0.1666\n",
      "CLIP Score: 0.1588\n",
      "CLIP Score: 0.1604\n",
      "CLIP Score: 0.1578\n",
      "CLIP Score: 0.1506\n",
      "CLIP Score: 0.1612\n",
      "CLIP Score: 0.1445\n",
      "CLIP Score: 0.1494\n",
      "CLIP Score: 0.1405\n",
      "CLIP Score: 0.1727\n",
      "CLIP Score: 0.1528\n",
      "CLIP Score: 0.1802\n",
      "CLIP Score: 0.1467\n",
      "CLIP Score: 0.1649\n",
      "CLIP Score: 0.1549\n",
      "CLIP Score: 0.1444\n",
      "CLIP Score: 0.1414\n",
      "CLIP Score: 0.1706\n",
      "CLIP Score: 0.1518\n",
      "CLIP Score: 0.1479\n",
      "CLIP Score: 0.1578\n",
      "CLIP Score: 0.1717\n",
      "CLIP Score: 0.1539\n",
      "CLIP Score: 0.1661\n",
      "CLIP Score: 0.1623\n",
      "CLIP Score: 0.1454\n",
      "CLIP Score: 0.1618\n",
      "CLIP Score: 0.1503\n",
      "CLIP Score: 0.1716\n",
      "CLIP Score: 0.1506\n",
      "Epoch 1/1 completed, Loss: 0.2381, CLIP Score: 0.1506\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "scaler = GradScaler()\n",
    "def calculate_clip_score(clip_model, generated_images, text_embeddings):\n",
    "    # Ensure generated images are in the format expected by CLIP (e.g., normalized, resized)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # CLIP expects 224x224 images\n",
    "    ])\n",
    "    \n",
    "    # If images are in latent space, convert them to the appropriate format\n",
    "    generated_images = preprocess(generated_images)\n",
    "\n",
    "    # Encode the generated images into CLIP image embeddings\n",
    "    image_embeddings = clip_model.encode_image(generated_images).float()\n",
    "\n",
    "    # Normalize both embeddings\n",
    "    image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "    text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute cosine similarity (CLIP Score)\n",
    "    clip_score = F.cosine_similarity(image_embeddings, text_embeddings, dim=-1)\n",
    "    return clip_score.mean().item()\n",
    "\n",
    "# Set up smaller UNet model (without .half())\n",
    "small_unet = UNet2DConditionModel(\n",
    "    sample_size=64,  # Reduce resolution to 64x64\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    layers_per_block=1,\n",
    "    block_out_channels=(64, 128, 256),  # Reduce the number of channels\n",
    "    down_block_types=('CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D'),\n",
    "    up_block_types=('UpBlock2D', 'UpBlock2D', 'UpBlock2D'),\n",
    "    attention_head_dim=32,  # Reduce attention heads for memory efficiency\n",
    "    cross_attention_dim=768  # CLIP model dimension\n",
    ")\n",
    "\n",
    "# Ensure UNet and VAE are on the correct device (no .half())\n",
    "pipe.unet = small_unet.to(device)\n",
    "pipe.unet.enable_gradient_checkpointing()  # Enable gradient checkpointing to save memory\n",
    "pipe.vae = pipe.vae.to(device)\n",
    "\n",
    "# Dataloader with reduced batch size\n",
    "dataloader = DataLoader(small_dataset, batch_size=4, shuffle=True)  # Reduce batch size for memory efficiency\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = torch.optim.AdamW(pipe.unet.parameters(), lr=0.00001)\n",
    "clip_loss_fn = torch.nn.CosineEmbeddingLoss()  # CLIP-based perceptual loss\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    for images, captions in dataloader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Generate text embeddings using fine-tuned CLIP\n",
    "        text_embeddings = model.encode_text(clip.tokenize(captions, truncate=True).to(device))\n",
    "\n",
    "        with autocast():  # Mixed precision context\n",
    "            # Encode the images into latent space using the VAE\n",
    "            latents = pipe.vae.encode(images).latent_dist.sample().to(device)\n",
    "\n",
    "            # Add noise to the latent representations\n",
    "            noise = torch.randn_like(latents).to(device)\n",
    "\n",
    "            # Sample a random timestep\n",
    "            timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "\n",
    "            # Pass the noisy latents and text embeddings to the UNet\n",
    "            generated_latents = pipe.unet(noise, timesteps, encoder_hidden_states=text_embeddings.unsqueeze(1).repeat(1, 77, 1)).sample\n",
    "\n",
    "            # Decode the generated latents back into image space using the VAE decoder\n",
    "            decoded_images = pipe.vae.decode(generated_latents).sample\n",
    "\n",
    "            # Compute diffusion loss (e.g., MSE between generated images and real images)\n",
    "            diffusion_loss = torch.nn.functional.mse_loss(decoded_images, images)\n",
    "\n",
    "            # Optionally add a CLIP-based perceptual loss\n",
    "            decoded_image_embeddings = model.encode_image(decoded_images)\n",
    "\n",
    "            # CLIP loss based on cosine similarity\n",
    "            target = torch.ones(decoded_image_embeddings.shape[0], device=device)  # CosineEmbeddingLoss expects target 1 for matching pairs\n",
    "            clip_loss = clip_loss_fn(decoded_image_embeddings, text_embeddings, target)\n",
    "\n",
    "            # Total loss: diffusion loss + CLIP perceptual loss\n",
    "            alpha = 0.8  # Adjust this weight to balance the two losses\n",
    "            loss = alpha * diffusion_loss + (1 - alpha) * clip_loss\n",
    "\n",
    "        # Backpropagation and optimization using mixed precision\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform backpropagation with scaled loss\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscale the gradients (letting GradScaler handle this)\n",
    "        scaler.unscale_(optimizer)\n",
    "\n",
    "        # Clip gradients to avoid exploding gradients (after unscaling)\n",
    "        torch.nn.utils.clip_grad_norm_(pipe.unet.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Step the optimizer\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Update the scaler for the next iteration\n",
    "        scaler.update()\n",
    "\n",
    "        # Calculate CLIP Score\n",
    "        with torch.no_grad():\n",
    "            clip_score = calculate_clip_score(model, decoded_images, text_embeddings)\n",
    "        print(f\"CLIP Score: {clip_score:.4f}\")\n",
    "\n",
    "        # Free memory at the end of the batch\n",
    "        del images, latents, generated_latents, decoded_images\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed, Loss: {loss.item():.4f}, CLIP Score: {clip_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0467198a-d868-42bb-8401-2a22ac8fd5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned models saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned models\n",
    "torch.save(pipe.unet.state_dict(), 'fine_tuned_unet.pth')\n",
    "torch.save(pipe.vae.state_dict(), 'fine_tuned_vae.pth')\n",
    "print(\"Fine-tuned models saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fd8060d-0607-4a15-a59a-b1eeb3d5b862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned models loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned models\n",
    "pipe.unet.load_state_dict(torch.load('fine_tuned_unet.pth'))\n",
    "pipe.vae.load_state_dict(torch.load('fine_tuned_vae.pth'))\n",
    "\n",
    "# Ensure the models are moved to the appropriate device\n",
    "pipe.unet.to(device)\n",
    "pipe.vae.to(device)\n",
    "print(\"Fine-tuned models loaded.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
